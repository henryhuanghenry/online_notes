<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="generator" content="VuePress 2.0.0-beta.46">
    <style>
      :root {
        --c-bg: #fff;
      }
      html.dark {
        --c-bg: #22272e;
      }
      html, body {
        background-color: var(--c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem('vuepress-color-scheme');
			const systemDarkMode = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
			if (userMode === 'dark' || (userMode !== 'light' && systemDarkMode)) {
				document.documentElement.classList.toggle('dark', true);
			}
    </script>
    <title>神经网络--RNN|LSTM|GRU | Yusheng Huang's page</title><meta name="description" content="">
    <link rel="modulepreload" href="/online_notes/assets/app.d92355f5.js"><link rel="modulepreload" href="/online_notes/assets/神经网络--RNN_LSTM_GRU.html.68804d13.js"><link rel="modulepreload" href="/online_notes/assets/神经网络--RNN_LSTM_GRU.html.3f96dea4.js"><link rel="prefetch" href="/online_notes/assets/index.html.62fd7aa5.js"><link rel="prefetch" href="/online_notes/assets/index.html.4584e283.js"><link rel="prefetch" href="/online_notes/assets/index.html.b94151cb.js"><link rel="prefetch" href="/online_notes/assets/动态规划.html.a52521a7.js"><link rel="prefetch" href="/online_notes/assets/回溯算法.html.1851ae13.js"><link rel="prefetch" href="/online_notes/assets/index.html.0175ac51.js"><link rel="prefetch" href="/online_notes/assets/simple_resume_En.html.764e588a.js"><link rel="prefetch" href="/online_notes/assets/simple_resume_Zh.html.fb7ace2e.js"><link rel="prefetch" href="/online_notes/assets/index.html.99583cda.js"><link rel="prefetch" href="/online_notes/assets/神经网络--Normalization Layers.html.4f22bccd.js"><link rel="prefetch" href="/online_notes/assets/神经网络--优化器.html.5f24fbe4.js"><link rel="prefetch" href="/online_notes/assets/线性模型.html.63d3670d.js"><link rel="prefetch" href="/online_notes/assets/评价指标.html.ce368a3b.js"><link rel="prefetch" href="/online_notes/assets/集成学习.html.b266a86b.js"><link rel="prefetch" href="/online_notes/assets/index.html.ca3b9443.js"><link rel="prefetch" href="/online_notes/assets/哈工大DB-第1讲初步认识数据库.html.3cb5055b.js"><link rel="prefetch" href="/online_notes/assets/index.html.3fb72605.js"><link rel="prefetch" href="/online_notes/assets/常用的数据结构和方法.html.a3a29efa.js"><link rel="prefetch" href="/online_notes/assets/404.html.f166316b.js"><link rel="prefetch" href="/online_notes/assets/index.html.fcf78575.js"><link rel="prefetch" href="/online_notes/assets/index.html.c2b95161.js"><link rel="prefetch" href="/online_notes/assets/index.html.f36db193.js"><link rel="prefetch" href="/online_notes/assets/动态规划.html.69f52e1c.js"><link rel="prefetch" href="/online_notes/assets/回溯算法.html.4db4ef4c.js"><link rel="prefetch" href="/online_notes/assets/index.html.b816c186.js"><link rel="prefetch" href="/online_notes/assets/simple_resume_En.html.df99b473.js"><link rel="prefetch" href="/online_notes/assets/simple_resume_Zh.html.46a1941e.js"><link rel="prefetch" href="/online_notes/assets/index.html.7295a36d.js"><link rel="prefetch" href="/online_notes/assets/神经网络--Normalization Layers.html.4ed8e140.js"><link rel="prefetch" href="/online_notes/assets/神经网络--优化器.html.a7981a77.js"><link rel="prefetch" href="/online_notes/assets/线性模型.html.8837be38.js"><link rel="prefetch" href="/online_notes/assets/评价指标.html.0d2171e7.js"><link rel="prefetch" href="/online_notes/assets/集成学习.html.93e4aa91.js"><link rel="prefetch" href="/online_notes/assets/index.html.a7ce3abf.js"><link rel="prefetch" href="/online_notes/assets/哈工大DB-第1讲初步认识数据库.html.ebb89c2a.js"><link rel="prefetch" href="/online_notes/assets/index.html.18580370.js"><link rel="prefetch" href="/online_notes/assets/常用的数据结构和方法.html.43560050.js"><link rel="prefetch" href="/online_notes/assets/404.html.1b196f1c.js"><link rel="prefetch" href="/online_notes/assets/404.03a4b7e1.js"><link rel="prefetch" href="/online_notes/assets/Layout.5d8b091d.js">
    <link rel="stylesheet" href="/online_notes/assets/style.86b505ac.css">
  </head>
  <body>
    <div id="app"><!--[--><div class="theme-container"><!--[--><header class="navbar"><div class="toggle-sidebar-button" title="toggle sidebar" aria-expanded="false" role="button" tabindex="0"><div class="icon" aria-hidden="true"><span></span><span></span><span></span></div></div><span><a href="/online_notes/" class=""><!----><span class="site-name">Yusheng Huang&#39;s page</span></a></span><div class="navbar-items-wrapper" style=""><!--[--><!--]--><nav class="navbar-items can-hide"><!--[--><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="基础课程笔记"><span class="title">基础课程笔记</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="基础课程笔记"><span class="title">基础课程笔记</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a href="/online_notes/CSclass/CSclass_DB/README.md" class="" aria-label="数据库"><!--[--><!--]--> 数据库 <!--[--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><a href="/online_notes/Algorithm/README.md" class="" aria-label="算法"><!--[--><!--]--> 算法 <!--[--><!--]--></a></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="AI"><span class="title">AI</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="AI"><span class="title">AI</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a href="/online_notes/AI/基础/README.md" class="" aria-label="基础"><!--[--><!--]--> 基础 <!--[--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="编程"><span class="title">编程</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="编程"><span class="title">编程</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a href="/online_notes/Programming/python/README.md" class="" aria-label="基础"><!--[--><!--]--> 基础 <!--[--><!--]--></a></li><!--]--></ul></div></div><!--]--></nav><!--[--><!--]--><button class="toggle-dark-button" title="toggle dark mode"><svg style="" class="icon" focusable="false" viewBox="0 0 32 32"><path d="M16 12.005a4 4 0 1 1-4 4a4.005 4.005 0 0 1 4-4m0-2a6 6 0 1 0 6 6a6 6 0 0 0-6-6z" fill="currentColor"></path><path d="M5.394 6.813l1.414-1.415l3.506 3.506L8.9 10.318z" fill="currentColor"></path><path d="M2 15.005h5v2H2z" fill="currentColor"></path><path d="M5.394 25.197L8.9 21.691l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 25.005h2v5h-2z" fill="currentColor"></path><path d="M21.687 23.106l1.414-1.415l3.506 3.506l-1.414 1.414z" fill="currentColor"></path><path d="M25 15.005h5v2h-5z" fill="currentColor"></path><path d="M21.687 8.904l3.506-3.506l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 2.005h2v5h-2z" fill="currentColor"></path></svg><svg style="display:none;" class="icon" focusable="false" viewBox="0 0 32 32"><path d="M13.502 5.414a15.075 15.075 0 0 0 11.594 18.194a11.113 11.113 0 0 1-7.975 3.39c-.138 0-.278.005-.418 0a11.094 11.094 0 0 1-3.2-21.584M14.98 3a1.002 1.002 0 0 0-.175.016a13.096 13.096 0 0 0 1.825 25.981c.164.006.328 0 .49 0a13.072 13.072 0 0 0 10.703-5.555a1.01 1.01 0 0 0-.783-1.565A13.08 13.08 0 0 1 15.89 4.38A1.015 1.015 0 0 0 14.98 3z" fill="currentColor"></path></svg></button><!----></div></header><!--]--><div class="sidebar-mask"></div><!--[--><aside class="sidebar"><nav class="navbar-items"><!--[--><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="基础课程笔记"><span class="title">基础课程笔记</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="基础课程笔记"><span class="title">基础课程笔记</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a href="/online_notes/CSclass/CSclass_DB/README.md" class="" aria-label="数据库"><!--[--><!--]--> 数据库 <!--[--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><a href="/online_notes/Algorithm/README.md" class="" aria-label="算法"><!--[--><!--]--> 算法 <!--[--><!--]--></a></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="AI"><span class="title">AI</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="AI"><span class="title">AI</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a href="/online_notes/AI/基础/README.md" class="" aria-label="基础"><!--[--><!--]--> 基础 <!--[--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="编程"><span class="title">编程</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="编程"><span class="title">编程</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a href="/online_notes/Programming/python/README.md" class="" aria-label="基础"><!--[--><!--]--> 基础 <!--[--><!--]--></a></li><!--]--></ul></div></div><!--]--></nav><!--[--><!--]--><ul class="sidebar-items"><!--[--><li><p tabindex="0" class="sidebar-item sidebar-heading active collapsible">AI基础 <span class="down arrow"></span></p><ul style="" class="sidebar-item-children"><!--[--><li><a href="/online_notes/AI/%E5%9F%BA%E7%A1%80/" class="router-link-active sidebar-item" aria-label="机器学习/深度学习--基础"><!--[--><!--]--> 机器学习/深度学习--基础 <!--[--><!--]--></a><!----></li><li><a href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--Normalization%20Layers.html" class="sidebar-item" aria-label="基础神经网络--Layer Norm"><!--[--><!--]--> 基础神经网络--Layer Norm <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html" class="router-link-active router-link-exact-active router-link-active sidebar-item active" aria-label="神经网络--RNN|LSTM|GRU"><!--[--><!--]--> 神经网络--RNN|LSTM|GRU <!--[--><!--]--></a><ul style="" class="sidebar-item-children"><!--[--><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_0-资料网址" class="router-link-active router-link-exact-active sidebar-item" aria-label="0.资料网址："><!--[--><!--]--> 0.资料网址： <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_1-rnn" class="router-link-active router-link-exact-active sidebar-item" aria-label="1. RNN"><!--[--><!--]--> 1. RNN <!--[--><!--]--></a><ul style="" class="sidebar-item-children"><!--[--><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_1-1-rnn的动机" class="router-link-active router-link-exact-active sidebar-item" aria-label="1.1 RNN的动机"><!--[--><!--]--> 1.1 RNN的动机 <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_1-2-rnn的结构和公式" class="router-link-active router-link-exact-active sidebar-item" aria-label="1.2 RNN的结构和公式"><!--[--><!--]--> 1.2 RNN的结构和公式 <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_1-3-rnn的优点、缺点" class="router-link-active router-link-exact-active sidebar-item" aria-label="1.3 RNN的优点、缺点"><!--[--><!--]--> 1.3 RNN的优点、缺点 <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_1-4-rnn中的梯度消失和爆炸" class="router-link-active router-link-exact-active sidebar-item" aria-label="1.4 ==RNN中的梯度消失和爆炸=="><!--[--><!--]--> 1.4 ==RNN中的梯度消失和爆炸== <!--[--><!--]--></a><!----></li><!--]--></ul></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_2-lstm" class="router-link-active router-link-exact-active sidebar-item" aria-label="2.LSTM"><!--[--><!--]--> 2.LSTM <!--[--><!--]--></a><ul style="" class="sidebar-item-children"><!--[--><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_2-1-lstm的动机" class="router-link-active router-link-exact-active sidebar-item" aria-label="2.1 LSTM的动机"><!--[--><!--]--> 2.1 LSTM的动机 <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_2-2-lstm的结构和公式" class="router-link-active router-link-exact-active sidebar-item" aria-label="2.2 LSTM的结构和公式"><!--[--><!--]--> 2.2 LSTM的结构和公式 <!--[--><!--]--></a><!----></li><!--]--></ul></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_3-gru" class="router-link-active router-link-exact-active sidebar-item" aria-label="3.GRU"><!--[--><!--]--> 3.GRU <!--[--><!--]--></a><ul style="" class="sidebar-item-children"><!--[--><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_3-1-gru的动机" class="router-link-active router-link-exact-active sidebar-item" aria-label="3.1 GRU的动机"><!--[--><!--]--> 3.1 GRU的动机 <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_3-2-gru的结构和公式" class="router-link-active router-link-exact-active sidebar-item" aria-label="3.2 GRU的结构和公式"><!--[--><!--]--> 3.2 GRU的结构和公式 <!--[--><!--]--></a><!----></li><!--]--></ul></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_4-其他" class="router-link-active router-link-exact-active sidebar-item" aria-label="4. 其他"><!--[--><!--]--> 4. 其他 <!--[--><!--]--></a><ul style="" class="sidebar-item-children"><!--[--><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_4-0-比较" class="router-link-active router-link-exact-active sidebar-item" aria-label="4.0 比较"><!--[--><!--]--> 4.0 比较 <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_4-1-常使用激活函数" class="router-link-active router-link-exact-active sidebar-item" aria-label="4.1 常使用激活函数"><!--[--><!--]--> 4.1 常使用激活函数 <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_4-2-参数量的计算" class="router-link-active router-link-exact-active sidebar-item" aria-label="4.2 参数量的计算"><!--[--><!--]--> 4.2 参数量的计算 <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_4-3-代码的书写" class="router-link-active router-link-exact-active sidebar-item" aria-label="4.3 代码的书写"><!--[--><!--]--> 4.3 代码的书写 <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_4-4-不同的整合方式" class="router-link-active router-link-exact-active sidebar-item" aria-label="4.4 不同的整合方式："><!--[--><!--]--> 4.4 不同的整合方式： <!--[--><!--]--></a><!----></li><!--]--></ul></li><!--]--></ul></li><li><a href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--%E4%BC%98%E5%8C%96%E5%99%A8.html" class="sidebar-item" aria-label="基础神经网络--优化器"><!--[--><!--]--> 基础神经网络--优化器 <!--[--><!--]--></a><!----></li><li><a href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B.html" class="sidebar-item" aria-label="线性模型"><!--[--><!--]--> 线性模型 <!--[--><!--]--></a><!----></li><li><a href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87.html" class="sidebar-item" aria-label="评价指标"><!--[--><!--]--> 评价指标 <!--[--><!--]--></a><!----></li><li><a href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0.html" class="sidebar-item" aria-label="集成学习"><!--[--><!--]--> 集成学习 <!--[--><!--]--></a><!----></li><!--]--></ul></li><!--]--></ul><!--[--><!--]--></aside><!--]--><!--[--><main class="page"><!--[--><!--]--><div class="theme-default-content"><!--[--><!--]--><div><h1 id="神经网络-rnn-lstm-gru" tabindex="-1"><a class="header-anchor" href="#神经网络-rnn-lstm-gru" aria-hidden="true">#</a> 神经网络--RNN|LSTM|GRU</h1><nav class="table-of-contents"><ul><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_0-资料网址" class="router-link-active router-link-exact-active">0.资料网址：</a></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_1-rnn" class="router-link-active router-link-exact-active">1. RNN</a><ul><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_1-1-rnn的动机" class="router-link-active router-link-exact-active">1.1 RNN的动机</a></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_1-2-rnn的结构和公式" class="router-link-active router-link-exact-active">1.2 RNN的结构和公式</a></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_1-3-rnn的优点、缺点" class="router-link-active router-link-exact-active">1.3 RNN的优点、缺点</a></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_1-4-rnn中的梯度消失和爆炸" class="router-link-active router-link-exact-active">1.4 ==RNN中的梯度消失和爆炸==</a></li></ul></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_2-lstm" class="router-link-active router-link-exact-active">2.LSTM</a><ul><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_2-1-lstm的动机" class="router-link-active router-link-exact-active">2.1 LSTM的动机</a></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_2-2-lstm的结构和公式" class="router-link-active router-link-exact-active">2.2 LSTM的结构和公式</a></li></ul></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_3-gru" class="router-link-active router-link-exact-active">3.GRU</a><ul><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_3-1-gru的动机" class="router-link-active router-link-exact-active">3.1 GRU的动机</a></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_3-2-gru的结构和公式" class="router-link-active router-link-exact-active">3.2 GRU的结构和公式</a></li></ul></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_4-其他" class="router-link-active router-link-exact-active">4. 其他</a><ul><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_4-0-比较" class="router-link-active router-link-exact-active">4.0 比较</a></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_4-1-常使用激活函数" class="router-link-active router-link-exact-active">4.1 常使用激活函数</a></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_4-2-参数量的计算" class="router-link-active router-link-exact-active">4.2 参数量的计算</a></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_4-3-代码的书写" class="router-link-active router-link-exact-active">4.3 代码的书写</a></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html#_4-4-不同的整合方式" class="router-link-active router-link-exact-active">4.4 不同的整合方式：</a></li></ul></li></ul></nav><h2 id="_0-资料网址" tabindex="-1"><a class="header-anchor" href="#_0-资料网址" aria-hidden="true">#</a> 0.资料网址：</h2><ul><li><a href="https://www.jiqizhixin.com/articles/2018-12-18-12" target="_blank" rel="noopener noreferrer">核心机器之心的post<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></li><li><a href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21" target="_blank" rel="noopener noreferrer">三个神经网络的动图解释<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></li><li><a href="https://paddlepedia.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener noreferrer">飞桨文档<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></li><li><a href="https://www.deeplearningbook.org/" target="_blank" rel="noopener noreferrer">Deep Learning Book<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></li><li><a href="https://www.youtube.com/c/3blue1brown" target="_blank" rel="noopener noreferrer">参考视频--可视化数学非常好的教学<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></li><li><a href="https://www.youtube.com/watch?v=CpD9XlTu3ys" target="_blank" rel="noopener noreferrer">SVD分解的图解视频<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></li><li><a href="https://easyai.tech/ai-definition/lstm/" target="_blank" rel="noopener noreferrer">介绍LSTM<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></li></ul><h2 id="_1-rnn" tabindex="-1"><a class="header-anchor" href="#_1-rnn" aria-hidden="true">#</a> 1. RNN</h2><h3 id="_1-1-rnn的动机" tabindex="-1"><a class="header-anchor" href="#_1-1-rnn的动机" aria-hidden="true">#</a> 1.1 RNN的动机</h3><ul><li>需要处理变长的序列</li><li>要学习到输入的时间的依赖关系</li></ul><h3 id="_1-2-rnn的结构和公式" tabindex="-1"><a class="header-anchor" href="#_1-2-rnn的结构和公式" aria-hidden="true">#</a> 1.2 RNN的结构和公式</h3><h4 id="结构" tabindex="-1"><a class="header-anchor" href="#结构" aria-hidden="true">#</a> 结构</h4><p><img src="/online_notes/assets/architecture-rnn-ltr.8683c721.png" alt="architecture-rnn-ltr"></p><ul><li>上述结构图参考(https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks#architecture)</li></ul><h4 id="公式" tabindex="-1"><a class="header-anchor" href="#公式" aria-hidden="true">#</a> 公式</h4><img src="/online_notes/assets/image-20220626143653270.0fd410fd.png" alt="image-20220626143653270" style="zoom:25%;"><h4 id="总结" tabindex="-1"><a class="header-anchor" href="#总结" aria-hidden="true">#</a> 总结</h4><ul><li><strong>RNN由输入、隐藏层、输出组成</strong></li><li><strong>输入是逐个输入</strong></li><li><strong>隐藏层--与当前输入、上一个隐藏层 有关</strong>，使用的激活函数为tanh <ul><li>__使用tanh__是因为在级联的时候，如果有一个输入值特别大，而没有tanh归一化[-1,1]的效果的话，连乘下去这个值到最后将会非常大，那么别的比较小的数就没有任何意义了</li></ul></li><li><strong>输出--只与当前隐藏层有关</strong></li></ul><h3 id="_1-3-rnn的优点、缺点" tabindex="-1"><a class="header-anchor" href="#_1-3-rnn的优点、缺点" aria-hidden="true">#</a> 1.3 RNN的优点、缺点</h3><table><thead><tr><th>优点:</th><th>缺点：</th></tr></thead><tbody><tr><td>模型大小不随输入长度的增加</td><td>计算缓慢</td></tr><tr><td>可以处理任意长序列</td><td>存在梯度消失和梯度爆炸的问题</td></tr><tr><td>考虑了时间的依赖</td><td></td></tr><tr><td>参数随时间共享</td><td></td></tr></tbody></table><h3 id="_1-4-rnn中的梯度消失和爆炸" tabindex="-1"><a class="header-anchor" href="#_1-4-rnn中的梯度消失和爆炸" aria-hidden="true">#</a> 1.4 ==RNN中的梯度消失和爆炸==</h3><img src="/online_notes/assets/image-20220626162631825.408e51de.png" alt="image-20220626162631825" style="zoom:33%;"><ul><li><strong>梯度爆炸：学习到的参数矩阵的最大奇异值大于1</strong><ul><li>可以使用梯度裁剪解决(clipping)</li></ul></li><li><strong>梯度消失：学习到的参数矩阵的最大奇异值小于1</strong><ul><li>当最大奇异值小于1的时候，往前传播的时候，随着t越来越大，梯度越来越小，越往前神经网络越不更新，削弱了RNN捕获长距离的能力</li></ul></li></ul><h2 id="_2-lstm" tabindex="-1"><a class="header-anchor" href="#_2-lstm" aria-hidden="true">#</a> 2.LSTM</h2><h3 id="_2-1-lstm的动机" tabindex="-1"><a class="header-anchor" href="#_2-1-lstm的动机" aria-hidden="true">#</a> 2.1 LSTM的动机</h3><p>__有选择性的输入__以缓解梯度消失的问题</p><h3 id="_2-2-lstm的结构和公式" tabindex="-1"><a class="header-anchor" href="#_2-2-lstm的结构和公式" aria-hidden="true">#</a> 2.2 LSTM的结构和公式</h3><h4 id="结构-1" tabindex="-1"><a class="header-anchor" href="#结构-1" aria-hidden="true">#</a> 结构</h4><img src="/online_notes/assets/1yBXV9o5q7L_CvY7quJt3WQ.552687f4.png" alt="img" style="zoom:50%;"><p><img src="/online_notes/assets/image-20210511204551220.a0c0d504.png" alt="image-20210511204551220"></p><h4 id="公式-注意-公式中激活函数内的相加不是相加-而是向量拼接" tabindex="-1"><a class="header-anchor" href="#公式-注意-公式中激活函数内的相加不是相加-而是向量拼接" aria-hidden="true">#</a> 公式--注意，公式中激活函数内的相加不是相加，而是向量拼接</h4><img src="/online_notes/assets/image-20220626172906049.30199249.png" alt="image-20220626172906049" style="zoom:80%;"><h4 id="总结-3门3态" tabindex="-1"><a class="header-anchor" href="#总结-3门3态" aria-hidden="true">#</a> 总结--3门3态</h4><ul><li><p><strong>门</strong>：</p><ul><li>都与 当前输入 和 上一时刻隐藏层 相关</li><li>都使用sigmoid作为激活函数</li><li>都有可学习参数</li></ul></li><li><p>输入门</p><ul><li>用于 细胞态 中调控 暂态 的输入</li></ul></li><li><p>遗忘门</p><ul><li>用于 细胞态 中调控 上一时刻 细胞态 的输入</li></ul></li><li><p>输出门</p><ul><li>用于从 细胞态 生成 当前时刻的 隐藏层</li></ul></li><li><p><strong>态</strong>：</p></li><li><p>暂态</p><ul><li>与 当前输入 和 上一时刻隐藏层 相关</li><li>使用tanh作为激活函数</li><li>有可学习参数</li></ul></li><li><p>细胞态</p><ul><li>由 暂态 与 上一时刻细胞态 决定</li><li>由 输入门 和 遗忘门 调控输入</li><li>无激活函数</li><li>无可学习参数</li></ul></li><li><p>隐藏层</p><ul><li>直接由 细胞态 经过 tanh 后经由 输出门 调控生成</li><li>使用tanh作为激活函数</li><li>无可学习参数</li></ul></li></ul><h2 id="_3-gru" tabindex="-1"><a class="header-anchor" href="#_3-gru" aria-hidden="true">#</a> 3.GRU</h2><h3 id="_3-1-gru的动机" tabindex="-1"><a class="header-anchor" href="#_3-1-gru的动机" aria-hidden="true">#</a> 3.1 GRU的动机</h3><h3 id="_3-2-gru的结构和公式" tabindex="-1"><a class="header-anchor" href="#_3-2-gru的结构和公式" aria-hidden="true">#</a> 3.2 GRU的结构和公式</h3><h4 id="结构-2" tabindex="-1"><a class="header-anchor" href="#结构-2" aria-hidden="true">#</a> 结构</h4><img src="/online_notes/assets/1yBXV9o5q7L_CvY7quJt3WQ.552687f4.png" alt="img" style="zoom:50%;"><img src="/online_notes/assets/gru.0b775ff9.png" alt="gru" style="zoom:50%;"><h4 id="公式-注意-公式中激活函数内的相加不是相加-而是向量拼接-1" tabindex="-1"><a class="header-anchor" href="#公式-注意-公式中激活函数内的相加不是相加-而是向量拼接-1" aria-hidden="true">#</a> 公式--注意，公式中激活函数内的相加不是相加，而是向量拼接</h4><img src="/online_notes/assets/image-20220626172842386.ba5521fe.png" alt="image-20220626172842386" style="zoom:80%;"><h4 id="总结-2门2态" tabindex="-1"><a class="header-anchor" href="#总结-2门2态" aria-hidden="true">#</a> 总结--2门2态</h4><ul><li><p><strong>门</strong>：</p><ul><li>都与当前输入和上一时刻的隐藏层有关，对于 当前输入 和 上一时刻隐藏层 都有可学习参数</li><li>都使用sigmoid激活函数进行逐个元素相乘，相当于门控</li></ul></li><li><p>重置门</p><ul><li>负责在 暂态 中遗忘 上一时刻的隐藏层</li></ul></li><li><p>更新门</p><ul><li>负责在 隐藏层 中调控 暂态 和 上一时刻隐藏层 的比例</li></ul></li><li><p><strong>态</strong>：</p><ul><li>GRU只有暂态和隐藏层</li></ul></li><li><p>暂态</p><ul><li>和RNN的隐藏层类似，只是 参数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∗</mo></mrow><annotation encoding="application/x-tex">*</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4653em;"></span><span class="mord">∗</span></span></span></span>​​​上一时刻隐藏层 需要经 重置门 调控</li><li>tanh为激活函数</li><li>有可学习参数</li></ul></li><li><p>隐藏层</p><ul><li>由 暂态 和 上一时刻隐藏层 经由 更新门 调控输入比例</li><li>无激活函数</li><li>无可学习参数</li></ul></li></ul><h2 id="_4-其他" tabindex="-1"><a class="header-anchor" href="#_4-其他" aria-hidden="true">#</a> 4. 其他</h2><h3 id="_4-0-比较" tabindex="-1"><a class="header-anchor" href="#_4-0-比较" aria-hidden="true">#</a> 4.0 比较</h3><h4 id="lstm与gru" tabindex="-1"><a class="header-anchor" href="#lstm与gru" aria-hidden="true">#</a> LSTM与GRU</h4><ul><li><p>LSTM能够解决循环神经网络因长期依赖带来的梯度消失和梯度爆炸问题，但是LSTM有三个不同的门，参数较多，训练起来比较困难。</p></li><li><p>GRU只含有两个门控结构，且在超参数全部调优的情况下，二者性能相当，但是GRU结构更为简单，训练样本较少，易实现。</p></li><li><img src="/online_notes/assets/image-20220626174043183.2c16c685.png" alt="image-20220626174043183"></li></ul><h3 id="_4-1-常使用激活函数" tabindex="-1"><a class="header-anchor" href="#_4-1-常使用激活函数" aria-hidden="true">#</a> 4.1 常使用激活函数</h3><img src="/online_notes/assets/image-20220626145349858.1f0ad3b5.png" alt="image-20220626145349858" style="zoom:80%;"><h4 id="为何非门控单元的可学习参数-选择tanh而不也是sigmoid" tabindex="-1"><a class="header-anchor" href="#为何非门控单元的可学习参数-选择tanh而不也是sigmoid" aria-hidden="true">#</a> 为何非门控单元的可学习参数，选择tanh而不也是sigmoid</h4><ul><li>在输入为0附近，tanh的梯度比sigmoid更大，学习收敛更快</li></ul><h4 id="可否使用relu作为激活函数" tabindex="-1"><a class="header-anchor" href="#可否使用relu作为激活函数" aria-hidden="true">#</a> 可否使用RELU作为激活函数</h4><ul><li>使用RELU作为激活函数，则失去了tanh的约束，会引发梯度的消失和爆炸</li><li>如果可学习参数初始化在单位阵附近，则可能可以使用</li></ul><h3 id="_4-2-参数量的计算" tabindex="-1"><a class="header-anchor" href="#_4-2-参数量的计算" aria-hidden="true">#</a> 4.2 参数量的计算</h3><p>参考pytorch中的计算：https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html?highlight=lstm#torch.nn.LSTM</p><h3 id="_4-3-代码的书写" tabindex="-1"><a class="header-anchor" href="#_4-3-代码的书写" aria-hidden="true">#</a> 4.3 代码的书写</h3><ul><li><a href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#LSTM" target="_blank" rel="noopener noreferrer">官方代码<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></li></ul><h3 id="_4-4-不同的整合方式" tabindex="-1"><a class="header-anchor" href="#_4-4-不同的整合方式" aria-hidden="true">#</a> 4.4 不同的整合方式：</h3><p>https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks#overview</p><img src="/online_notes/assets/image-20220626174913378.3c239a04.png" alt="image-20220626174913378" style="zoom:50%;"></div><!--[--><!--]--></div><footer class="page-meta"><!----><div class="meta-item last-updated"><span class="meta-item-label">Last Updated: </span><!----></div><div class="meta-item contributors"><span class="meta-item-label">Contributors: </span><span class="meta-item-info"><!--[--><!--[--><span class="contributor" title="email: henryhuanghenry@outlook.com">henryhuang</span><!----><!--]--><!--]--></span></div></footer><nav class="page-nav"><p class="inner"><span class="prev"><a href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--Normalization%20Layers.html" class="" aria-label="基础神经网络--Layer Norm"><!--[--><!--]--> 基础神经网络--Layer Norm <!--[--><!--]--></a></span><span class="next"><a href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--%E4%BC%98%E5%8C%96%E5%99%A8.html" class="" aria-label="基础神经网络--优化器"><!--[--><!--]--> 基础神经网络--优化器 <!--[--><!--]--></a></span></p></nav><!--[--><!--]--></main><!--]--></div><!----><!--]--></div>
    <script type="module" src="/online_notes/assets/app.d92355f5.js" defer></script>
  </body>
</html>
