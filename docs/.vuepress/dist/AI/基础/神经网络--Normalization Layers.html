<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="generator" content="VuePress 2.0.0-beta.46">
    <style>
      :root {
        --c-bg: #fff;
      }
      html.dark {
        --c-bg: #22272e;
      }
      html, body {
        background-color: var(--c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem('vuepress-color-scheme');
			const systemDarkMode = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
			if (userMode === 'dark' || (userMode !== 'light' && systemDarkMode)) {
				document.documentElement.classList.toggle('dark', true);
			}
    </script>
    <title>基础神经网络--Layer Norm | Yusheng Huang's page</title><meta name="description" content="">
    <link rel="modulepreload" href="/online_notes/assets/app.d92355f5.js"><link rel="modulepreload" href="/online_notes/assets/神经网络--Normalization Layers.html.4ed8e140.js"><link rel="modulepreload" href="/online_notes/assets/神经网络--Normalization Layers.html.4f22bccd.js"><link rel="prefetch" href="/online_notes/assets/index.html.62fd7aa5.js"><link rel="prefetch" href="/online_notes/assets/index.html.4584e283.js"><link rel="prefetch" href="/online_notes/assets/index.html.b94151cb.js"><link rel="prefetch" href="/online_notes/assets/动态规划.html.a52521a7.js"><link rel="prefetch" href="/online_notes/assets/回溯算法.html.1851ae13.js"><link rel="prefetch" href="/online_notes/assets/index.html.0175ac51.js"><link rel="prefetch" href="/online_notes/assets/simple_resume_En.html.764e588a.js"><link rel="prefetch" href="/online_notes/assets/simple_resume_Zh.html.fb7ace2e.js"><link rel="prefetch" href="/online_notes/assets/index.html.99583cda.js"><link rel="prefetch" href="/online_notes/assets/神经网络--RNN_LSTM_GRU.html.3f96dea4.js"><link rel="prefetch" href="/online_notes/assets/神经网络--优化器.html.5f24fbe4.js"><link rel="prefetch" href="/online_notes/assets/线性模型.html.63d3670d.js"><link rel="prefetch" href="/online_notes/assets/评价指标.html.ce368a3b.js"><link rel="prefetch" href="/online_notes/assets/集成学习.html.b266a86b.js"><link rel="prefetch" href="/online_notes/assets/index.html.ca3b9443.js"><link rel="prefetch" href="/online_notes/assets/哈工大DB-第1讲初步认识数据库.html.3cb5055b.js"><link rel="prefetch" href="/online_notes/assets/index.html.3fb72605.js"><link rel="prefetch" href="/online_notes/assets/常用的数据结构和方法.html.a3a29efa.js"><link rel="prefetch" href="/online_notes/assets/404.html.f166316b.js"><link rel="prefetch" href="/online_notes/assets/index.html.fcf78575.js"><link rel="prefetch" href="/online_notes/assets/index.html.c2b95161.js"><link rel="prefetch" href="/online_notes/assets/index.html.f36db193.js"><link rel="prefetch" href="/online_notes/assets/动态规划.html.69f52e1c.js"><link rel="prefetch" href="/online_notes/assets/回溯算法.html.4db4ef4c.js"><link rel="prefetch" href="/online_notes/assets/index.html.b816c186.js"><link rel="prefetch" href="/online_notes/assets/simple_resume_En.html.df99b473.js"><link rel="prefetch" href="/online_notes/assets/simple_resume_Zh.html.46a1941e.js"><link rel="prefetch" href="/online_notes/assets/index.html.7295a36d.js"><link rel="prefetch" href="/online_notes/assets/神经网络--RNN_LSTM_GRU.html.68804d13.js"><link rel="prefetch" href="/online_notes/assets/神经网络--优化器.html.a7981a77.js"><link rel="prefetch" href="/online_notes/assets/线性模型.html.8837be38.js"><link rel="prefetch" href="/online_notes/assets/评价指标.html.0d2171e7.js"><link rel="prefetch" href="/online_notes/assets/集成学习.html.93e4aa91.js"><link rel="prefetch" href="/online_notes/assets/index.html.a7ce3abf.js"><link rel="prefetch" href="/online_notes/assets/哈工大DB-第1讲初步认识数据库.html.ebb89c2a.js"><link rel="prefetch" href="/online_notes/assets/index.html.18580370.js"><link rel="prefetch" href="/online_notes/assets/常用的数据结构和方法.html.43560050.js"><link rel="prefetch" href="/online_notes/assets/404.html.1b196f1c.js"><link rel="prefetch" href="/online_notes/assets/404.03a4b7e1.js"><link rel="prefetch" href="/online_notes/assets/Layout.5d8b091d.js">
    <link rel="stylesheet" href="/online_notes/assets/style.86b505ac.css">
  </head>
  <body>
    <div id="app"><!--[--><div class="theme-container"><!--[--><header class="navbar"><div class="toggle-sidebar-button" title="toggle sidebar" aria-expanded="false" role="button" tabindex="0"><div class="icon" aria-hidden="true"><span></span><span></span><span></span></div></div><span><a href="/online_notes/" class=""><!----><span class="site-name">Yusheng Huang&#39;s page</span></a></span><div class="navbar-items-wrapper" style=""><!--[--><!--]--><nav class="navbar-items can-hide"><!--[--><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="基础课程笔记"><span class="title">基础课程笔记</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="基础课程笔记"><span class="title">基础课程笔记</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a href="/online_notes/CSclass/CSclass_DB/README.md" class="" aria-label="数据库"><!--[--><!--]--> 数据库 <!--[--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><a href="/online_notes/Algorithm/README.md" class="" aria-label="算法"><!--[--><!--]--> 算法 <!--[--><!--]--></a></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="AI"><span class="title">AI</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="AI"><span class="title">AI</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a href="/online_notes/AI/基础/README.md" class="" aria-label="基础"><!--[--><!--]--> 基础 <!--[--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="编程"><span class="title">编程</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="编程"><span class="title">编程</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a href="/online_notes/Programming/python/README.md" class="" aria-label="基础"><!--[--><!--]--> 基础 <!--[--><!--]--></a></li><!--]--></ul></div></div><!--]--></nav><!--[--><!--]--><button class="toggle-dark-button" title="toggle dark mode"><svg style="" class="icon" focusable="false" viewBox="0 0 32 32"><path d="M16 12.005a4 4 0 1 1-4 4a4.005 4.005 0 0 1 4-4m0-2a6 6 0 1 0 6 6a6 6 0 0 0-6-6z" fill="currentColor"></path><path d="M5.394 6.813l1.414-1.415l3.506 3.506L8.9 10.318z" fill="currentColor"></path><path d="M2 15.005h5v2H2z" fill="currentColor"></path><path d="M5.394 25.197L8.9 21.691l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 25.005h2v5h-2z" fill="currentColor"></path><path d="M21.687 23.106l1.414-1.415l3.506 3.506l-1.414 1.414z" fill="currentColor"></path><path d="M25 15.005h5v2h-5z" fill="currentColor"></path><path d="M21.687 8.904l3.506-3.506l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 2.005h2v5h-2z" fill="currentColor"></path></svg><svg style="display:none;" class="icon" focusable="false" viewBox="0 0 32 32"><path d="M13.502 5.414a15.075 15.075 0 0 0 11.594 18.194a11.113 11.113 0 0 1-7.975 3.39c-.138 0-.278.005-.418 0a11.094 11.094 0 0 1-3.2-21.584M14.98 3a1.002 1.002 0 0 0-.175.016a13.096 13.096 0 0 0 1.825 25.981c.164.006.328 0 .49 0a13.072 13.072 0 0 0 10.703-5.555a1.01 1.01 0 0 0-.783-1.565A13.08 13.08 0 0 1 15.89 4.38A1.015 1.015 0 0 0 14.98 3z" fill="currentColor"></path></svg></button><!----></div></header><!--]--><div class="sidebar-mask"></div><!--[--><aside class="sidebar"><nav class="navbar-items"><!--[--><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="基础课程笔记"><span class="title">基础课程笔记</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="基础课程笔记"><span class="title">基础课程笔记</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a href="/online_notes/CSclass/CSclass_DB/README.md" class="" aria-label="数据库"><!--[--><!--]--> 数据库 <!--[--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><a href="/online_notes/Algorithm/README.md" class="" aria-label="算法"><!--[--><!--]--> 算法 <!--[--><!--]--></a></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="AI"><span class="title">AI</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="AI"><span class="title">AI</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a href="/online_notes/AI/基础/README.md" class="" aria-label="基础"><!--[--><!--]--> 基础 <!--[--><!--]--></a></li><!--]--></ul></div></div><div class="navbar-item"><div class="navbar-dropdown-wrapper"><button class="navbar-dropdown-title" type="button" aria-label="编程"><span class="title">编程</span><span class="arrow down"></span></button><button class="navbar-dropdown-title-mobile" type="button" aria-label="编程"><span class="title">编程</span><span class="right arrow"></span></button><ul style="display:none;" class="navbar-dropdown"><!--[--><li class="navbar-dropdown-item"><a href="/online_notes/Programming/python/README.md" class="" aria-label="基础"><!--[--><!--]--> 基础 <!--[--><!--]--></a></li><!--]--></ul></div></div><!--]--></nav><!--[--><!--]--><ul class="sidebar-items"><!--[--><li><p tabindex="0" class="sidebar-item sidebar-heading active collapsible">AI基础 <span class="down arrow"></span></p><ul style="" class="sidebar-item-children"><!--[--><li><a href="/online_notes/AI/%E5%9F%BA%E7%A1%80/" class="router-link-active sidebar-item" aria-label="机器学习/深度学习--基础"><!--[--><!--]--> 机器学习/深度学习--基础 <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--Normalization%20Layers.html" class="router-link-active router-link-exact-active router-link-active sidebar-item active" aria-label="基础神经网络--Layer Norm"><!--[--><!--]--> 基础神经网络--Layer Norm <!--[--><!--]--></a><ul style="" class="sidebar-item-children"><!--[--><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--Normalization%20Layers.html#_0-资料网址" class="router-link-active router-link-exact-active sidebar-item" aria-label="0.资料网址："><!--[--><!--]--> 0.资料网址： <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--Normalization%20Layers.html#归一化的通用框架" class="router-link-active router-link-exact-active sidebar-item" aria-label="归一化的通用框架"><!--[--><!--]--> 归一化的通用框架 <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--Normalization%20Layers.html#_1-bn-一个batch进行scale" class="router-link-active router-link-exact-active sidebar-item" aria-label="1. BN -- 一个batch进行scale"><!--[--><!--]--> 1. BN -- 一个batch进行scale <!--[--><!--]--></a><ul style="" class="sidebar-item-children"><!--[--><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--Normalization%20Layers.html#_1-1-为什么需要bn" class="router-link-active router-link-exact-active sidebar-item" aria-label="1.1 为什么需要BN"><!--[--><!--]--> 1.1 为什么需要BN <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--Normalization%20Layers.html#_1-2-bn的算法" class="router-link-active router-link-exact-active sidebar-item" aria-label="1.2 BN的算法"><!--[--><!--]--> 1.2 BN的算法 <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--Normalization%20Layers.html#_1-3-评估的时候" class="router-link-active router-link-exact-active sidebar-item" aria-label="1.3 评估的时候"><!--[--><!--]--> 1.3 评估的时候 <!--[--><!--]--></a><!----></li><!--]--></ul></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--Normalization%20Layers.html#_2-layer-norm-一层scale" class="router-link-active router-link-exact-active sidebar-item" aria-label="2. Layer Norm -- 一层scale"><!--[--><!--]--> 2. Layer Norm -- 一层scale <!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--Normalization%20Layers.html#_3-weight-norm-使用权重对数据scale" class="router-link-active router-link-exact-active sidebar-item" aria-label="3. Weight Norm -- 使用权重对数据scale"><!--[--><!--]--> 3. Weight Norm -- 使用权重对数据scale <!--[--><!--]--></a><!----></li><!--]--></ul></li><li><a href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html" class="sidebar-item" aria-label="神经网络--RNN|LSTM|GRU"><!--[--><!--]--> 神经网络--RNN|LSTM|GRU <!--[--><!--]--></a><!----></li><li><a href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--%E4%BC%98%E5%8C%96%E5%99%A8.html" class="sidebar-item" aria-label="基础神经网络--优化器"><!--[--><!--]--> 基础神经网络--优化器 <!--[--><!--]--></a><!----></li><li><a href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B.html" class="sidebar-item" aria-label="线性模型"><!--[--><!--]--> 线性模型 <!--[--><!--]--></a><!----></li><li><a href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87.html" class="sidebar-item" aria-label="评价指标"><!--[--><!--]--> 评价指标 <!--[--><!--]--></a><!----></li><li><a href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0.html" class="sidebar-item" aria-label="集成学习"><!--[--><!--]--> 集成学习 <!--[--><!--]--></a><!----></li><!--]--></ul></li><!--]--></ul><!--[--><!--]--></aside><!--]--><!--[--><main class="page"><!--[--><!--]--><div class="theme-default-content"><!--[--><!--]--><div><h1 id="基础神经网络-layer-norm" tabindex="-1"><a class="header-anchor" href="#基础神经网络-layer-norm" aria-hidden="true">#</a> 基础神经网络--Layer Norm</h1><nav class="table-of-contents"><ul><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--Normalization%20Layers.html#_0-资料网址" class="router-link-active router-link-exact-active">0.资料网址：</a></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--Normalization%20Layers.html#归一化的通用框架" class="router-link-active router-link-exact-active">归一化的通用框架</a></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--Normalization%20Layers.html#_1-bn-一个batch进行scale" class="router-link-active router-link-exact-active">1. BN -- 一个batch进行scale</a><ul><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--Normalization%20Layers.html#_1-1-为什么需要bn" class="router-link-active router-link-exact-active">1.1 为什么需要BN</a></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--Normalization%20Layers.html#_1-2-bn的算法" class="router-link-active router-link-exact-active">1.2 BN的算法</a></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--Normalization%20Layers.html#_1-3-评估的时候" class="router-link-active router-link-exact-active">1.3 评估的时候</a></li></ul></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--Normalization%20Layers.html#_2-layer-norm-一层scale" class="router-link-active router-link-exact-active">2. Layer Norm -- 一层scale</a></li><li><a aria-current="page" href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--Normalization%20Layers.html#_3-weight-norm-使用权重对数据scale" class="router-link-active router-link-exact-active">3. Weight Norm -- 使用权重对数据scale</a></li></ul></nav><h2 id="_0-资料网址" tabindex="-1"><a class="header-anchor" href="#_0-资料网址" aria-hidden="true">#</a> 0.资料网址：</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/33173246" target="_blank" rel="noopener noreferrer">知乎关于norm的好专题文章<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></li></ul><h2 id="归一化的通用框架" tabindex="-1"><a class="header-anchor" href="#归一化的通用框架" aria-hidden="true">#</a> 归一化的通用框架</h2><img src="/online_notes/assets/image-20220628205443548.986a210c.png" alt="image-20220628205443548" style="zoom:67%;"><h2 id="_1-bn-一个batch进行scale" tabindex="-1"><a class="header-anchor" href="#_1-bn-一个batch进行scale" aria-hidden="true">#</a> 1. BN -- 一个batch进行scale</h2><img src="/online_notes/assets/image-20220628205603236.92dd9d02.png" alt="image-20220628205603236" style="zoom:50%;"><ul><li><a href="http://proceedings.mlr.press/v37/ioffe15.pdf" target="_blank" rel="noopener noreferrer">论文Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></li><li><a href="https://blog.csdn.net/happynear/article/details/44238541" target="_blank" rel="noopener noreferrer">好的阅读笔记<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></li><li><a href="https://proceedings.neurips.cc/paper/2018/file/36072923bfc3cf47745d704feb489480-Paper.pdf" target="_blank" rel="noopener noreferrer">论文Understanding Batch Normalization<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></li></ul><h3 id="_1-1-为什么需要bn" tabindex="-1"><a class="header-anchor" href="#_1-1-为什么需要bn" aria-hidden="true">#</a> 1.1 为什么需要BN</h3><ul><li><p>BN是对于每个激活函数的操作(可能在激活函数之前，也可能在激活函数之后)</p></li><li><p>需要BN的原因是所谓的Internal Covariate Shift</p><ul><li><img src="/online_notes/assets/image-20220628112432023-16563866733421.2e57468f.png" alt="image-20220628112432023" style="zoom:50%;"></li><li>ICS导致了如下的问题： <ul><li>其一，上层参数需要不断适应新的输入数据分布，降低学习速度。</li><li>其二，下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。 <ul><li><img src="/online_notes/assets/image-20220628113609658-16563873709792.44dc6814.png" alt="image-20220628113609658" style="zoom:50%;"></li></ul></li><li>其三，每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。</li></ul></li></ul></li></ul><h3 id="_1-2-bn的算法" tabindex="-1"><a class="header-anchor" href="#_1-2-bn的算法" aria-hidden="true">#</a> 1.2 BN的算法</h3><img src="/online_notes/assets/image-20220628113933578-16563875756703.9aa05953.png" alt="image-20220628113933578" style="zoom:67%;"><ul><li>即对每个激活函数的输入/输出x，将x先Z归一化成y，而后把y替换掉x。</li><li>而这个归一化，是使用一个batch内x的均值和方差来进行的，因此叫batch norm</li><li>而为了给网络更大的能力，设置了两个可学习的函数，把Z归一化后的y再scale和bias，成为最终的替代</li></ul><h3 id="_1-3-评估的时候" tabindex="-1"><a class="header-anchor" href="#_1-3-评估的时候" aria-hidden="true">#</a> 1.3 评估的时候</h3><img src="/online_notes/assets/image-20220628114419631-16563878611514.5c770ed3.png" alt="image-20220628114419631" style="zoom:67%;"> - 在训练的最后一个epoch，需要记录下所有mini-batch的每个的mean和Var - 最后统计得出这个激活函数对整个epoch的mean和Var。在评估过程中，使用整体mean和Var进行归一化 <h2 id="_2-layer-norm-一层scale" tabindex="-1"><a class="header-anchor" href="#_2-layer-norm-一层scale" aria-hidden="true">#</a> 2. Layer Norm -- 一层scale</h2><img src="/online_notes/assets/image-20220628205641253.77ccdd4f.png" alt="image-20220628205641253" style="zoom:67%;"><ul><li><a href="https://arxiv.org/pdf/1607.06450.pdf" target="_blank" rel="noopener noreferrer">论文Layer Normalization<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></li></ul><h2 id="_3-weight-norm-使用权重对数据scale" tabindex="-1"><a class="header-anchor" href="#_3-weight-norm-使用权重对数据scale" aria-hidden="true">#</a> 3. Weight Norm -- 使用权重对数据scale</h2><img src="/online_notes/assets/image-20220628205754749.d49dd7f1.png" alt="image-20220628205754749" style="zoom:67%;"><img src="/online_notes/assets/image-20220628205943844.b63172fc.png" alt="image-20220628205943844" style="zoom:67%;"><img src="/online_notes/assets/image-20220628210135431.4c8253be.png" alt="image-20220628210135431" style="zoom:67%;"></div><!--[--><!--]--></div><footer class="page-meta"><!----><div class="meta-item last-updated"><span class="meta-item-label">Last Updated: </span><!----></div><div class="meta-item contributors"><span class="meta-item-label">Contributors: </span><span class="meta-item-info"><!--[--><!--[--><span class="contributor" title="email: henryhuanghenry@outlook.com">henryhuang</span><!----><!--]--><!--]--></span></div></footer><nav class="page-nav"><p class="inner"><span class="prev"><a href="/online_notes/AI/%E5%9F%BA%E7%A1%80/" class="router-link-active" aria-label="机器学习/深度学习--基础"><!--[--><!--]--> 机器学习/深度学习--基础 <!--[--><!--]--></a></span><span class="next"><a href="/online_notes/AI/%E5%9F%BA%E7%A1%80/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--RNN+LSTM+GRU.html" class="" aria-label="神经网络--RNN|LSTM|GRU"><!--[--><!--]--> 神经网络--RNN|LSTM|GRU <!--[--><!--]--></a></span></p></nav><!--[--><!--]--></main><!--]--></div><!----><!--]--></div>
    <script type="module" src="/online_notes/assets/app.d92355f5.js" defer></script>
  </body>
</html>
